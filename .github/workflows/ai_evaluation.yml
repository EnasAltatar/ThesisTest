name: AI Evaluation Pipeline

on:
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Run in test mode (first 5 notes only)'
        required: true
        type: boolean
        default: true
      batch_size:
        description: 'Number of notes to process per batch'
        required: false
        type: number
        default: 50

jobs:
  evaluate-notes:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install openai pandas openpyxl numpy scipy scikit-learn matplotlib seaborn
      
      - name: Verify input files
        run: |
          echo "Checking for required files..."
          echo "Current directory: $(pwd)"
          echo "Repository contents:"
          ls -la
          echo ""
          echo "Data directory contents:"
          ls -lh data/ || { echo "‚ùå Error: data/ directory not found"; exit 1; }
          echo ""
          if [ ! -f "data/khcc_eval_input.xlsx" ]; then
            echo "‚ùå Error: khcc_eval_input.xlsx not found in data/ directory"
            exit 1
          fi
          echo "‚úì Input file found: data/khcc_eval_input.xlsx"
      
      - name: Configure evaluation mode
        run: |
          if [ "${{ inputs.test_mode }}" = "true" ]; then
            echo "üß™ Running in TEST MODE"
            sed -i 's/TEST_MODE = False/TEST_MODE = True/' scripts/ai_evaluator.py
          else
            echo "üöÄ Running in FULL MODE"
            sed -i 's/TEST_MODE = True/TEST_MODE = False/' scripts/ai_evaluator.py
          fi
      
      - name: Run AI evaluation
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python scripts/ai_evaluator_gpt.py
      
      - name: Upload evaluation results
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: ai-evaluation-results-${{ github.run_number }}
          path: |
            outputs/ai_evaluations.csv
            outputs/ai_evaluations_full.json
            outputs/ai_evaluations_summary.csv
          retention-days: 90
      
      - name: Create summary comment
        if: success()
        run: |
          echo "# AI Evaluation Complete! üéâ" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run Number:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Download your results from the Artifacts section below." >> $GITHUB_STEP_SUMMARY
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå AI Evaluation Pipeline Failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Check the logs above for error details." >> $GITHUB_STEP_SUMMARY

  compare-with-human:
    runs-on: ubuntu-latest
    needs: evaluate-notes
    if: github.event_name == 'workflow_dispatch' && contains(github.event.inputs.test_mode, 'false')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scipy scikit-learn matplotlib seaborn
      
      - name: Download AI evaluation results
        uses: actions/download-artifact@v4
        with:
          name: ai-evaluation-results-${{ github.run_number }}
          path: outputs/
      
      - name: Check for human evaluations
        id: check_human
        run: |
          if [ -f "data/human_evaluations.csv" ]; then
            echo "human_exists=true" >> $GITHUB_OUTPUT
            echo "‚úì Human evaluation file found"
          else
            echo "human_exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è  Human evaluation file not found - skipping comparison"
          fi
      
      - name: Compare evaluations
        if: steps.check_human.outputs.human_exists == 'true'
        run: |
          cd scripts
          python compare_evaluations.py
      
      - name: Upload comparison results
        if: steps.check_human.outputs.human_exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-comparison-${{ github.run_number }}
          path: |
            outputs/comparison_results/
          retention-days: 90
