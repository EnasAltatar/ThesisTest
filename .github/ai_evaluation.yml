name: AI Evaluation Pipeline

on:
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Run in test mode (first 5 notes only)'
        required: true
        type: boolean
        default: true
      batch_size:
        description: 'Number of notes to process per batch'
        required: false
        type: number
        default: 50

jobs:
  evaluate-notes:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install anthropic pandas openpyxl numpy scipy scikit-learn matplotlib seaborn
      
      - name: Verify input files
        run: |
          echo "Checking for required files..."
          ls -lh data/
          if [ ! -f "data/khcc_eval_input.xlsx" ]; then
            echo "‚ùå Error: khcc_eval_input.xlsx not found in data/ directory"
            exit 1
          fi
          echo "‚úì Input file found"
      
      - name: Configure evaluation mode
        run: |
          if [ "${{ inputs.test_mode }}" = "true" ]; then
            echo "üß™ Running in TEST MODE"
            sed -i 's/TEST_MODE = False/TEST_MODE = True/' scripts/ai_evaluator.py
          else
            echo "üöÄ Running in FULL MODE"
            sed -i 's/TEST_MODE = True/TEST_MODE = False/' scripts/ai_evaluator.py
          fi
      
      - name: Run AI evaluation
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          cd scripts
          python ai_evaluator.py
      
      - name: Generate summary report
        if: success()
        run: |
          echo "# AI Evaluation Summary" > evaluation_summary.md
          echo "" >> evaluation_summary.md
          echo "**Run Date:** $(date)" >> evaluation_summary.md
          echo "**Test Mode:** ${{ inputs.test_mode }}" >> evaluation_summary.md
          echo "" >> evaluation_summary.md
          
          if [ -f "outputs/ai_evaluations_summary.csv" ]; then
            echo "## Score Statistics" >> evaluation_summary.md
            echo "\`\`\`" >> evaluation_summary.md
            cat outputs/ai_evaluations_summary.csv >> evaluation_summary.md
            echo "\`\`\`" >> evaluation_summary.md
          fi
          
          if [ -f "outputs/ai_evaluations.csv" ]; then
            TOTAL_NOTES=$(tail -n +2 outputs/ai_evaluations.csv | wc -l)
            echo "" >> evaluation_summary.md
            echo "**Total Notes Evaluated:** $TOTAL_NOTES" >> evaluation_summary.md
          fi
      
      - name: Upload evaluation results
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: ai-evaluation-results-${{ github.run_number }}
          path: |
            outputs/ai_evaluations.csv
            outputs/ai_evaluations_full.json
            outputs/ai_evaluations_summary.csv
            evaluation_summary.md
          retention-days: 90
      
      - name: Commit results to repository
        if: success() && github.event_name == 'workflow_dispatch'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          mkdir -p results/$(date +%Y%m%d)
          cp outputs/ai_evaluations.csv results/$(date +%Y%m%d)/
          cp outputs/ai_evaluations_full.json results/$(date +%Y%m%d)/
          cp outputs/ai_evaluations_summary.csv results/$(date +%Y%m%d)/
          cp evaluation_summary.md results/$(date +%Y%m%d)/
          
          git add results/
          git commit -m "Add AI evaluation results - Run #${{ github.run_number }}" || echo "No changes to commit"
          git push || echo "Nothing to push"
      
      - name: Create summary comment
        if: success()
        run: |
          cat evaluation_summary.md >> $GITHUB_STEP_SUMMARY
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå AI Evaluation Pipeline Failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Check the logs above for error details." >> $GITHUB_STEP_SUMMARY

  compare-with-human:
    runs-on: ubuntu-latest
    needs: evaluate-notes
    if: github.event_name == 'workflow_dispatch' && contains(github.event.inputs.test_mode, 'false')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scipy scikit-learn matplotlib seaborn
      
      - name: Download AI evaluation results
        uses: actions/download-artifact@v4
        with:
          name: ai-evaluation-results-${{ github.run_number }}
          path: outputs/
      
      - name: Check for human evaluations
        id: check_human
        run: |
          if [ -f "data/human_evaluations.csv" ]; then
            echo "human_exists=true" >> $GITHUB_OUTPUT
            echo "‚úì Human evaluation file found"
          else
            echo "human_exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è  Human evaluation file not found - skipping comparison"
          fi
      
      - name: Compare evaluations
        if: steps.check_human.outputs.human_exists == 'true'
        run: |
          cd scripts
          python compare_evaluations.py
      
      - name: Upload comparison results
        if: steps.check_human.outputs.human_exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-comparison-${{ github.run_number }}
          path: |
            outputs/comparison_results/
          retention-days: 90
